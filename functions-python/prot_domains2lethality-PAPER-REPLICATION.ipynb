{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication of results from paper: \"Predicting yeast synthetic lethal genetic interactions using protein domains\" \n",
    "\n",
    "- Authors: Bo Li, Feng Luo,School of Computing,Clemson University,Clemson, SC, USA\n",
    "- e-mail: bol, luofeng@clemson.edu\n",
    "- year:2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict \n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import scipy as scipy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing datasets \n",
    "\n",
    "#### Link to the github repo where the datasets to be downloaded:\n",
    "[DOWNLOAD THE DATASETS HERE](https://github.com/leilaicruz/machine-learning-for-yeast/tree/dev_Leila/datasets-for-learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_dir = os.path.dirname('__file__') #<-- absolute dir the script is in\n",
    "rel_path_SL = \"datasets/data-synthetic-lethals.xlsx\"\n",
    "rel_path_nSL=\"datasets/data-positive-genetic.xlsx\"\n",
    "rel_path_domains=\"datasets/proteins-domains-from-Pfam.xlsx\"\n",
    "\n",
    "abs_file_path_SL = os.path.join(script_dir, rel_path_SL)\n",
    "abs_file_path_nSL = os.path.join(script_dir, rel_path_nSL)\n",
    "abs_file_path_domains = os.path.join(script_dir, rel_path_domains)\n",
    "\n",
    "os.chdir('../') #<-- for binder os.chdir('../')\n",
    "my_path_sl= abs_file_path_SL\n",
    "my_path_non_sl=abs_file_path_nSL\n",
    "my_path_domains=abs_file_path_domains\n",
    "\n",
    "data_sl=pd.read_excel(my_path_sl,header=0)\n",
    "data_domains=pd.read_excel(my_path_domains,header=0,index_col='Unnamed: 0')\n",
    "data_domains=data_domains.dropna()\n",
    "data_nonsl=pd.read_excel(my_path_non_sl,header=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the feature matrix\n",
    "One matrix for true SL where each row is one pair of SL. Every raw will be a vector of 0,1 or 2 depending on the comparison with the domain list. For row i the jth element = 0 if the jth element of the domain list is not in neither protein A and B, 1, if it is in one of them and 2 if it is in both of them .\n",
    "\n",
    "### Building the list of proteins domains id per protein pair separately :\n",
    "- List of protein A: Search for the Sl/nSL database the query gene name and look in the protein domain database which protein domains id has each of those queries.\n",
    "- List of protein B: Search for the Sl/nSL database the target gene name of the previous query and look in the protein domain database which protein domains id has each of those target genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the meaningful columns in the respective dataset\n",
    "domain_id_list=data_domains['domain-name']\n",
    "query_gene=data_sl['gene-query-name']\n",
    "target_gene=data_sl['gene-target-name']\n",
    "query_gene_nonlethal=data_nonsl['gene-query-name']\n",
    "target_gene_nonlethal=data_nonsl['gene-target-name']\n",
    "\n",
    "\n",
    "\n",
    "# Initialising the arrays\n",
    "protein_a_list=[]\n",
    "protein_b_list=[]\n",
    "protein_a_list_non=[]\n",
    "protein_b_list_non=[]\n",
    "\n",
    "population = np.arange(0,len(data_sl))\n",
    "\n",
    "# For loop for 10000 pairs sampled randomly from the SL/nSl pair list , and creating a big array of proteind domains id per protein pair\n",
    "\n",
    "for m in random.sample(list(population), 10000):\n",
    "    protein_a=data_domains[data_domains['name']==query_gene[m]]\n",
    "    protein_b=data_domains[data_domains['name']==target_gene[m]]\n",
    "    protein_a_list.append(protein_a['domain-name'].tolist())\n",
    "    protein_b_list.append(protein_b['domain-name'].tolist())\n",
    "\n",
    "    protein_a_non=data_domains[data_domains['name']==query_gene_nonlethal[m]]\n",
    "    protein_b_non=data_domains[data_domains['name']==target_gene_nonlethal[m]]\n",
    "    protein_a_list_non.append(protein_a_non['domain-name'].tolist())\n",
    "    protein_b_list_non.append(protein_b_non['domain-name'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We are going to analyze',len((protein_a_list)) ,'protein pairs, out of',len(data_sl),'SL protein pairs')\n",
    "print('We are going to analyze',len((protein_a_list_non)) ,'protein pairs, out of',len(data_nonsl),'positive protein pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing #1:  Remove protein pairs from study if either protein in the pair does not contain any domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_domains(protein_list_search,protein_list_pair):\n",
    "    index=[]\n",
    "    for i in np.arange(0,len(protein_list_search)):\n",
    "        if protein_list_search[i]==[]:\n",
    "            index.append(i) ## index of empty values for the protein_a_list meaning they dont have any annotated domain\n",
    "\n",
    "    y=[x for x in np.arange(0,len(protein_list_search)) if x not in index] # a list with non empty values from protein_a list\n",
    "\n",
    "    protein_list_search_new=[]\n",
    "    protein_list_pair_new=[]\n",
    "    for i in y:\n",
    "        protein_list_search_new.append(protein_list_search[i])\n",
    "        protein_list_pair_new.append(protein_list_pair[i])\n",
    "    return protein_list_search_new,protein_list_pair_new\n",
    "\n",
    "## evaluating the function\n",
    "\n",
    "protein_a_list_new,protein_b_list_new=remove_empty_domains(protein_a_list,protein_b_list)\n",
    "protein_a_list_non_new,protein_b_list_non_new=remove_empty_domains(protein_a_list_non,protein_b_list_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The empty domain in the SL were:', len(protein_a_list)-len(protein_a_list_new), 'out of', len(protein_a_list),'domains')\n",
    "print('The empty domain in the nSL were:', len(protein_a_list_non)-len(protein_a_list_non_new), 'out of', len(protein_a_list_non),'domains')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering: Select from each ordered indexes of domain id list which of them appear once, in both or in any of the domains of each protein pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function `get_indexes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1, 2]"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "get_indexes = lambda x, xs: [i for (y, i) in zip(xs, range(len(xs))) if x == y] # a function that give the index of whether a value appear in array or not\n",
    "a=[1,2,2,4,5,6,7,8,9,10]\n",
    "get_indexes(2,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_building(protein_a_list_new,protein_b_list_new):\n",
    "    x = np.unique(domain_id_list)\n",
    "    protein_feat_true=np.zeros(shape=(len(x),len(protein_a_list_new)))\n",
    "    pair_a_b_array=[]\n",
    "    for i in np.arange(0,len(protein_a_list_new)):\n",
    "        index_a=[]\n",
    "        pair=[protein_a_list_new[i],protein_b_list_new[i]]\n",
    "        pair_a_b=np.concatenate(pair).ravel()\n",
    "        pair_a_b_array.append(pair_a_b)\n",
    "\n",
    "    for i in np.arange(0,len(pair_a_b_array)):  \n",
    "        array,index,counts=np.unique(pair_a_b_array[i],return_index=True,return_counts=True)\n",
    "        for k,m in zip(counts,array):\n",
    "            if k ==2:\n",
    "                protein_feat_true[get_indexes(m,x),i]=2\n",
    "                \n",
    "            if k==1:\n",
    "                protein_feat_true[get_indexes(m,x),i]=1\n",
    "            # print(index_a[m],i)\n",
    "    return protein_feat_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_feat_true=feature_building(protein_b_list_new=protein_b_list_new,protein_a_list_new=protein_a_list_new)\n",
    "protein_feat_true_pd=pd.DataFrame(protein_feat_true.T)\n",
    "\n",
    "protein_feat_non_true=feature_building(protein_b_list_new=protein_b_list_non_new,protein_a_list_new=protein_a_list_non_new)\n",
    "protein_feat_non_true_pd=pd.DataFrame(protein_feat_non_true.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many ones and twos are in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2_true=protein_feat_true_pd.where(protein_feat_true_pd==2)\n",
    "index_2_true_count=index_2_true.count(axis=1).sum()\n",
    "\n",
    "index_1_true=protein_feat_true_pd.where(protein_feat_true_pd==1)\n",
    "index_1_true_count=index_1_true.count(axis=1).sum()\n",
    "\n",
    "index_2_nontrue=protein_feat_non_true_pd.where(protein_feat_non_true_pd==2)\n",
    "index_2_nontrue_count=index_2_nontrue.count(axis=1).sum()\n",
    "\n",
    "index_1_nontrue=protein_feat_non_true_pd.where(protein_feat_non_true_pd==1)\n",
    "index_1_nontrue_count=index_1_nontrue.count(axis=1).sum()\n",
    "\n",
    "\n",
    "print('fraction of twos in the SL array is',index_2_true_count/(len(protein_feat_true_pd.index)*len(protein_feat_true_pd.columns)))\n",
    "print('fraction of ones in the SL array is',index_1_true_count/(len(protein_feat_true_pd.index)*len(protein_feat_true_pd.columns)))\n",
    "print('fraction of twos in the PI array is',index_2_nontrue_count/(len(protein_feat_non_true_pd.index)*len(protein_feat_non_true_pd.columns)))\n",
    "print('fraction of ones in the PI array is',index_1_nontrue_count/(len(protein_feat_non_true_pd.index)*len(protein_feat_non_true_pd.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar plot to visualize these numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(['fraction of 2 in the nSL','fraction of 1 in the nSL'],[index_2_nontrue_count/(len(protein_feat_non_true_pd.index)*len(protein_feat_non_true_pd.columns)),index_1_nontrue_count/(len(protein_feat_non_true_pd.index)*len(protein_feat_non_true_pd.columns))],alpha=0.6,color=['blue','lightblue']), \n",
    "\n",
    "plt.bar(['fraction of 2 in SL ','fraction of 1 in SL'],[index_2_true_count/(len(protein_feat_true_pd.index)*len(protein_feat_true_pd.columns)),index_1_true_count/(len(protein_feat_true_pd.index)*len(protein_feat_true_pd.columns))],alpha=0.6,color=['coral','lightcoral'])\n",
    "\n",
    "plt.ylabel('Fraction from the population')\n",
    "plt.yscale('log')\n",
    "plt.xticks(rotation=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the labels(response variables) to each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_feat_true_pd['lethality']=np.ones(shape=(len(protein_a_list_new)))\n",
    "protein_feat_non_true_pd['lethality']=np.zeros(shape=(len(protein_a_list_non_new)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The number of features are: 3026\nThe number of samples are: 18622\n"
    }
   ],
   "source": [
    "feature_post=pd.concat([protein_feat_true_pd,protein_feat_non_true_pd],axis=0)\n",
    "feature_post=feature_post.set_index(np.arange(0,len(protein_a_list_new)+len(protein_a_list_non_new)))\n",
    "print('The number of features are:',feature_post.shape[1])\n",
    "print('The number of samples are:',feature_post.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing and exploration of the feature matrix of both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=feature_post.T.describe().loc['mean']\n",
    "std=feature_post.T.describe().loc['std']\n",
    "lethality=feature_post['lethality']\n",
    "\n",
    "corr_keys=pd.concat([mean,std,lethality],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz of the stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(10,5))\n",
    "a=sns.violinplot(x=\"lethality\", y=\"mean\", data=corr_keys,ax=axs[0],palette='colorblind')\n",
    "a.set_title('How the mean varies with Lethality')\n",
    "b=sns.violinplot(x=\"lethality\", y=\"std\", data=corr_keys,ax=axs[1],palette='colorblind')\n",
    "b.set_title('How the std varies with Lethality')\n",
    "##plt.savefig('violinplot-mean-std-with-lethality.png', format='png',dpi=300,transparent='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair=sns.pairplot(corr_keys,hue='lethality',diag_kind='kde',kind='reg',palette='colorblind')\n",
    "pair.fig.suptitle('Pairplot to see data dependencies with Lethality',y=1.08)\n",
    "##plt.savefig('Pairplot-to-see-data-dependencies-with-Lethality.png',format='png',dpi=300,transparent='True', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=scipy.stats.pearsonr(corr_keys['mean'],corr_keys['lethality'])\n",
    "p_value_corr=defaultdict(dict)\n",
    "\n",
    "columns=['mean','std']\n",
    "for i in columns:\n",
    "    \n",
    "    tmp=scipy.stats.pearsonr(corr_keys[i],corr_keys['lethality'])\n",
    "    p_value_corr[i]['corr with lethality']=tmp[0]\n",
    "    p_value_corr[i]['p-value']=tmp[1]\n",
    "\n",
    "p_value_corr_pd=pd.DataFrame(p_value_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr_keys.corr()\n",
    "sns.heatmap(corr, vmax=1,vmin=-1 ,square=True,cmap=cm.PRGn,cbar_kws={'label':'Pearson corr'})\n",
    "##plt.savefig('Heatmap-Pearson-corr-mean-std-lethality.png', format='png',dpi=300,transparent='true',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate features from labels to set up the data from the ML workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = feature_post.drop(columns=[\"lethality\"]), feature_post[\"lethality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train set: (13035, 3025) (13035,)\nTest set: (5587, 3025) (5587,)\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X,y,test_size = 0.3, random_state= 0)\n",
    "\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the best SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "parameters = [{'C': [1, 10, 100], 'kernel': ['rbf'], 'gamma': ['auto','scale']}]\n",
    "search = GridSearchCV(SVC(), parameters, n_jobs=-1, verbose=1)\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = search.best_estimator_\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with the best model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note : This learning is without applying reduction of dimensionality with PCA . This make the SVM not optimized at all. \n",
    "Later we shall see how this svm is affected after `PCA` transformation to the data . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(clf, '../model_SVC_C_10_gamma_scale_kernel_rbf_10000x3072_matrix.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "clf = load('../model_SVC_C_10_gamma_scale_kernel_rbf_10000x3072_matrix.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# print('Train set Accuracy: ', metrics.accuracy_score(y_train, clf.predict(X_train)))\n",
    "print('The mean squared error is =',metrics.mean_squared_error(y_test,y_pred))\n",
    "print('Test set Accuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    "print('The Jaccard index is =', jaccard_score(y_test, y_pred))\n",
    "# Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets. The closer to 1 the better the classifier \n",
    "print('The log-loss is =',log_loss(y_test,y_pred))\n",
    "# how far each prediction is from the actual label, it is like a distance measure from the predicted to the actual , the classifer with lower log loss have better accuracy\n",
    "print('The f1-score is =',metrics.f1_score(y_test,y_pred))\n",
    "# The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=['NonSl','SL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "scores=clf.decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, scores)\n",
    "area=metrics.auc(fpr,tpr)\n",
    "plt.plot(fpr,tpr,color='darkorange',label='SVM model (area = %0.2f)' % area)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',label='Random prediction')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.savefig('ROC-curve-SVC-on-classifing-lethality-using-PI-SL.png',format='png',dpi=300,transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = metrics.precision_recall_curve(y_test, scores)\n",
    "average_precision = metrics.average_precision_score(y_test, scores)\n",
    "plt.plot(precision,recall,color='blue',label='SVM-model')\n",
    "\n",
    "plt.plot([0.5, 1], [1, 0], color='navy', lw=2, linestyle='--',label='Random prediction')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))\n",
    "plt.legend()\n",
    "\n",
    "#plt.savefig('Precision-Recall-curve.png',format='png',dpi=300,transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=[1,2,3]\n",
    "fig, ax = plt.subplots()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred,normalize=\"true\")\n",
    "\n",
    "class_names=['SL', 'nSL']\n",
    "\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"Blues\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "#plt.savefig('confusion-matrix-normalized.png',format='png',dpi=300,transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step of crossvalidation to evaluate the peformance of the classifier in terms of overfitting \n",
    "\n",
    "(**Caution!**) Highly time consuming ~2h for 10000 X 3072 matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "t = time.process_time()\n",
    "cv=StratifiedKFold(n_splits=5)\n",
    "elapsed_time = time.process_time() - t\n",
    "print('The elapsed time was',elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "cv_results = cross_validate(clf, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the results \n",
    "dump(cv_results, '../cross_val_object_5_fold_clf_model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "#loading the crossvalidation \n",
    "cv=load('../cross_val_object_5_fold_clf_model.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz of the variation of the test error per fold . If the variation is high , the classifier may be proned to overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=1, figsize=(3,3))\n",
    "sorted(cv_results.keys())\n",
    "\n",
    "plt.scatter(['test-1','test-2','test-3','test-4','test-5'],cv_results['test_score'],s=60,alpha=0.7,color='blue')\n",
    "plt.title('5-fold crossvalidation result')\n",
    "plt.ylim(0.55,0.9)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('5-fold-crrosvalidation-result.png', format='png',dpi=300,transparent='true',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Using PCA to reduce the dimensionality of the problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "model_scaler = scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "x_train_S = model_scaler.transform(X_train)\n",
    "x_test_S = model_scaler.transform(X_test)\n",
    "\n",
    "# Fit PCA on training set. Note: you are fitting PCA on the training set only.\n",
    "model = PCA(0.95).fit(x_train_S)\n",
    "\n",
    "x_train_output_pca = model.transform(x_train_S)\n",
    "x_test_output_pca = model.transform(x_test_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(13035, 1622)"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "np.shape(x_train_output_pca)\n",
    "#np.shape(X_train)\n",
    "#model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(13035,)"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 61.7min finished\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GridSearchCV(cv=None, error_score=nan,\n             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n                           class_weight=None, coef0=0.0,\n                           decision_function_shape='ovr', degree=3,\n                           gamma='scale', kernel='rbf', max_iter=-1,\n                           probability=False, random_state=None, shrinking=True,\n                           tol=0.001, verbose=False),\n             iid='deprecated', n_jobs=-1,\n             param_grid=[{'C': [1, 10, 100], 'gamma': ['auto', 'scale'],\n                          'kernel': ['rbf']}],\n             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n             scoring=None, verbose=1)"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "parameters = [{'C': [1, 10, 100], 'kernel': ['rbf'], 'gamma': ['auto','scale']}]\n",
    "search = GridSearchCV(SVC(), parameters, n_jobs=-1, verbose=1)\n",
    "search.fit(x_train_output_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)\n"
    }
   ],
   "source": [
    "best_parameters = search.best_estimator_\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8018614641131198"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False).fit(x_train_output_pca, y_train)\n",
    "clf.score(x_test_output_pca, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['../model_SVC_C_10_gamma_scale_kernel_rbf_10000x1622_after_PCA_matrix.joblib']"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "clf_after_pca=clf\n",
    "dump(clf_after_pca, '../model_SVC_C_10_gamma_scale_kernel_rbf_10000x1622_after_PCA_matrix.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "y_pred_after_pca = clf_after_pca.predict(x_test_output_pca)\n",
    "\n",
    "# print('Train set Accuracy: ', metrics.accuracy_score(y_train, clf.predict(X_train)))\n",
    "print('The mean squared error is =',metrics.mean_squared_error(y_test,y_pred_after_pca))\n",
    "print('Test set Accuracy: ', metrics.accuracy_score(y_test, y_pred_after_pca))\n",
    "print('The Jaccard index is =', jaccard_score(y_test, y_pred_after_pca))\n",
    "# Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets. The closer to 1 the better the classifier \n",
    "print('The log-loss is =',log_loss(y_test,y_pred_after_pca))\n",
    "# how far each prediction is from the actual label, it is like a distance measure from the predicted to the actual , the classifer with lower log loss have better accuracy\n",
    "print('The f1-score is =',metrics.f1_score(y_test,y_pred_after_pca))\n",
    "# The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred_after_pca))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred_after_pca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}