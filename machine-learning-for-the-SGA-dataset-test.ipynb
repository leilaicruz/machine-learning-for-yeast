{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Classification of genetic interactions  using Support Vector Machine(SVM)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). \n",
    "The dataset comes from a proof-of-concept study published in 1999 by Golub et al. It showed how new cases of cancer could be classified by gene expression monitoring (via DNA microarray) and thereby provided a general approach for identifying new cancer classes and assigning tumors to known classes.\n",
    "The goal is to classify patients with acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) using the SVM algorithm.\n",
    "\n",
    "Lets start coding :)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############patients/genes########################################\n",
    "\n",
    "data_synthetic_lethals=pd.read_excel(r'C:\\Users\\linigodelacruz\\Documents\\PhD_2018\\Documentation\\Calculations\\Functions\\version-control-functions\\data-from-yeastmine-queries\\data_synthetic_lethals_from_list_yeastmine.xlsx',header=2)\n",
    "data_synthetic_lethals.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "## non synthetc lethals\n",
    "data_dosage_rescue=pd.read_excel(r'C:\\Users\\linigodelacruz\\Documents\\PhD_2018\\Documentation\\Calculations\\Functions\\version-control-functions\\data-from-yeastmine-queries\\data_dosage_rescue_from_list_yeastmine.xlsx',header=2)\n",
    "data_dosage_rescue.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "\n",
    "##################features(data to use for the prediction)###############################\n",
    "##curated data on slim go terms in budding yeast SGD downloads\n",
    "data_raw_slim_go=pd.read_excel(r'C:\\Users\\linigodelacruz\\Documents\\PhD_2018\\Documentation\\Calculations\\data_sgd\\slim-goterms-filtered-data.xlsx',header=0,encoding=\"utf-8-sig\")\n",
    "\n",
    "## published data in the cellmap.org 2016 \n",
    "data_fitness_sga=pd.read_excel(r'C:\\Users\\linigodelacruz\\Documents\\PhD_2018\\Documentation\\Calculations\\SGA-Boone-LAB\\Data File S1. Raw genetic interaction datasets_ Pair-wise interaction format\\Data-fitness.xlsx',header=0,sheet_name='NxN')\n",
    "\n",
    "## curated data set on interactions in budding yeast SGD downloads\n",
    "data_raw_interact=pd.read_excel(r'C:\\Users\\linigodelacruz\\Documents\\PhD_2018\\Documentation\\Calculations\\data_sgd\\interaction-filtered-data.xlsx',header=0,encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "data_raw_slim_go.columns=['Gene','gene-id','go-aspect','go-term','go-id','feature-type' ]\n",
    "data_raw_interact.columns=['Gene', 'Interactor', 'Assay', 'Annotation', 'Notes','Phenotype','Reference-SGD','citation']\n",
    "data_fitness_sga.columns=['query-strain','query-allele-name','array-strain','array-allele-name','array-type','score','p-value','query-fitness','array-fitness','double-fitness','double-fitness-std']\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=defaultdict(dict)\n",
    "target_gene='ACT1'\n",
    "\n",
    "synt_lethals_target_gene=data_synthetic_lethals[data_synthetic_lethals['name']==target_gene]['interactor']\n",
    "non_synt_lethals_target_gene=data_dosage_rescue[data_dosage_rescue['name']==target_gene]['interactor']\n",
    "\n",
    "\n",
    "for i in synt_lethals_target_gene:\n",
    "    counter=0\n",
    "    go_per_synt_gene=data_raw_slim_go[data_raw_slim_go['Gene']==i]['go-term'].tolist()\n",
    "\n",
    "    for j in go_per_synt_gene:\n",
    "        counter=counter+1\n",
    "        train_data[i][counter]=j\n",
    "        train_data[i]['lethal']=1\n",
    "    interactor_per_synt_gene=data_raw_interact[data_raw_interact['Gene']==i]['Interactor'].tolist()\n",
    "    for k in interactor_per_synt_gene:\n",
    "        counter=counter+1\n",
    "        train_data[i][counter]=k\n",
    "        train_data[i]['lethal']=1\n",
    "    \n",
    "\n",
    "for i in non_synt_lethals_target_gene:\n",
    "    counter=0 ## when adding new rows\n",
    "    go_per_non_synt_gene=data_raw_slim_go[data_raw_slim_go['Gene']==i]['go-term'].tolist()\n",
    "    for j in go_per_non_synt_gene:\n",
    "        counter=counter+1\n",
    "        train_data[i][counter]=j\n",
    "        train_data[i]['lethal']=0\n",
    "    interactor_per_non_synt_gene=data_raw_interact[data_raw_interact['Gene']==i]['Interactor'].tolist()\n",
    "    for k in interactor_per_non_synt_gene:\n",
    "        counter=counter+1\n",
    "        train_data[i][counter]=k\n",
    "        train_data[i]['lethal']=0\n",
    "\n",
    "\n",
    "train_data=pd.DataFrame(train_data)\n",
    "train_data=train_data.T.fillna('no-info')\n",
    "train_data.index=np.arange(0,len(synt_lethals_target_gene)+len(non_synt_lethals_target_gene)-1) \n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transpose, the rows have been converted to columns(7129 columns/features)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"patient\"] = np.arange(0,len(synt_lethals_target_gene)+len(non_synt_lethals_target_gene)-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to create two variables X(matrix of independent variables) and y(vector of the dependent variable)."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=train_data.drop(columns=['lethal']),train_data['lethal']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, \n",
    "We split 75% of the data into training set while 25% of the data to test set. \n",
    "The *test_size* variable is where we actually specify the proportion of the test set."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X,y,test_size = 0.25, random_state= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is :\n",
    "to normalize the data because if we closely look at the data the range of values of independent variables varies a lot.\n",
    "\n",
    "So when the values vary a lot in independent variables, we use **feature scaling** so that all the values remain in the comparable range."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "##########!!!!!!!!!!!!!!!!!ERROR!!!!!!!!!!!!!!!!########## because my features are strings, dont know how to solve this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of columns/features that we have been working with is huge. \n",
    "We have 72 rows and 7129 columns. \n",
    "Basically we need to decrease the number of features(Dimentioanlity Reduction) to remove the possibility of [ Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\n",
    "\n",
    "For reducing the number of dimensions/features we will use the most popular dimensionality reduction algorithm i.e. **PCA(Principal Component Analysis)**.\n",
    "To perform PCA we have to choose the number of features/dimensions that we want in our data."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA() \n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "total=sum(pca.explained_variance_)\n",
    "k=0\n",
    "current_variance=0\n",
    "while current_variance/total < 0.90:\n",
    "    current_variance += pca.explained_variance_[k]\n",
    "    k=k+1\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above code gives k=38.\n",
    "Now let us take k=38 and apply PCA on our independent variables."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 38)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "cum_sum = pca.explained_variance_ratio_.cumsum()\n",
    "cum_sum = cum_sum*100\n",
    "plt.bar(range(38), cum_sum)\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.xlabel(\"Principal Components\")\n",
    "plt.title(\"Around 90% of variance is explained by the First 38 columns \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:- PCA can lead to a reduction in model performance on datasets with no or low feature correlation or does not meet the assumptions of linearity.**\n",
    "The next step is to fit our data into the Support Vector Machine(SVM) algorithm but before doing that we will perform Hyperparameter optimization.\n",
    "\n",
    "> [Hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization) or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters are learned\n",
    "\n",
    "We will use GridSearchCV from sklearn for choosing the best hyperparameters."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "parameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "search = GridSearchCV(SVC(), parameters, n_jobs=-1, verbose=1)\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check what are the best parameters for our SVM algorithm"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = search.best_estimator_\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our SVM classification model."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma=0.001,\n",
    "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
    "    shrinking=True, tol=0.001, verbose=False)\n",
    "    \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time for some predictions!"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Evaluating model performance"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import metrics\n",
    "print('Accuracy Score:',round(accuracy_score(y_test, y_pred),2))\n",
    "#confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix and visualize it using Heatmap."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=[1,2,3]\n",
    "fig, ax = plt.subplots()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "class_names=['ALL', 'AML']\n",
    "\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"Blues\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this example goes to show that if you just predict that every patient has AML, you’ll be correct more often than wrong.\n",
    "\n",
    "So our SVM classification model predicted the cancer patients with 67% accuracy which is of course not that good.\n",
    "\n",
    "What you can do is try different classifiers like Random forest, K-NN, Gradient Boosting, xgboost etc and compare the accuracies for each model."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}